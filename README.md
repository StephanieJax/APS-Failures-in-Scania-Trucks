# readme-edits

Stephanie Jax, Ivan Ateyea, Colten Coffman, Echo Lee
Introduction to Machine Learning

Air Pressure System Failure Prediction in Scania Trucks

- Abstract -
This project explores air pressure system (APS) failures in Scania trucks. We leveraged machine learning models to predict how commonly the APS failure correlates to overall truck failure. Two sets were used: a training set consisting of 60,000 samples and a test set consisting of 16,000 samples. The datasets were a sizable variety of values across the board, so we filled those values with imputed means. Our model is structured around an optimal Random Forest Classifier. We evaluated the model with an F1 score, a ROC curve, and a confusion matrix. 
The Air Pressure System (APS) is a critical part of a heavy duty vehicle, such as the Scania Heavy Duty Trucks. In this system, air is compressed and causes the piston to apply pressure on the brake pads, which then decrease the speed of the vehicle to a stop. There are cases of APS failures during operation. The dataset used contains 60,000 examples of reported failures during operation of the trucks, of which we have a disproportionate number of failures, roughly 98% of examples, that are not related to the APS. This is the negative class, whereas the positive class for our binary classification problem is failures related to the APS. The test set that we are using for our machine learning model will consist of 16,000 examples. There are 171 anonymized attributes (170 features and 1 class label) which consist of numerical values and histogram variables. The histogram variables are split into different bins corresponding to temperature. The missing data is filled with imputed means corresponding to the general attribute values. Our goal is to predict APS failures preemptively to avoid costly realized failure.
- Methodology -
In order to determine which attributes of the dataset directly relate to our classifier (failure in the air pressure system), our data required considerable refinement. All of our data was already in numerical format so we did not have to transform any values.  However, accounting for missing values was a significant part in our preprocessing steps due to the imbalanced nature of the dataset.  Initially, we solved this by replacing ‘na’ values to numpy’s NaN values, then dropped the values from the pandas dataframe.  After doing this, our dataset was reduced by nearly 99%.  We did not want the missing values to skew our model, so we decided to fill the missing values with imputed values (replacing the missing values with a “mean” value of the nearest neighbors instead of getting rid of the row altogether).  We utilized the common SMOTE algorithm from the imblearn library to balance our minority instances.   Without this step, the model would overfit to the majority class compared to the minority- yielding unfavorable results and overfitting the model.  After the data preparation we used the Random Forest Classifier.  Tuned parameters were selected based on sci-kit learn documentation for default variables used as thresholds for splitting a node.  Hyperparameter tuning was done with cross validation so that the best parameters could be selected as predictors.  As hyperparameter tuning is a cumbersome art, we used Scikit-Learn’s built in RandomizedSearchCV which narrows down the best parameters through cross validation.  Each randomized search iteration was done using 5-fold cross validation.  The classifier was calibrated which identified the “best” parameters in our model, and is best practice especially when working with imbalance datasets.  
- Data Analysis -
Given our highly imbalanced dataset consisting of ~98% positive cases and only ~2% negative cases, we chose to use F1 score as our primary evaluation metric. F1 score computes the harmonic mean of precision and recall which allows for a more accurate representation of our misclassifications. The focus on misclassifications is particularly important considering the immense potential cost of a false negative. We created a confusion matrix to visually represent our results as well, with classes of 0 and 1 corresponding to positive and negative, respectively. In total, we saw 15, 544 true positives, 81 false positives, 126 false negatives, and 249 true negatives. This gave us an accuracy of ~98% for correct classifications with an F1 score of 0.71. For a final evaluation metric, we plotted the true positive rate against the false positive rate in an ROC curve. Based on the ROC curve and its distance from the random selection line, our model performed quite well. We compared our results to a basic KNN classification model. For the sake of efficiency, we omitted NaN values entirely as imputing means to fill them was incredibly computationally expensive without a similar architecture to our Random Forest Classification involving a pipeline. The KNN scored an accuracy of ~83%.
Conclusion:
In conclusion, we were able to build an accurate predictive model that is able to mitigate the cost of failures in Scania trucks. Our F1 score was worse than our accuracy score which is likely due to the heavy skew towards negative cases in the dataset. We sacrificed some potential upside in our evaluations for the sake of efficiency, but our general performance greatly outperforms a more simple model like KNN. We could have improved our F1 score with a greater number of iterations per Random Forest Classifier during model selection, as well, but at the cost of efficiency. 
